{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgFoomRMpERP5gP/NRHErG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seshveer07/project-1/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HotQpVzNi_ZQ"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Install Required Libraries\n",
        "!pip install transformers accelerate gradio --quiet\n",
        "\n",
        "# ‚úÖ Imports\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ‚úÖ Load IBM Granite Model\n",
        "model_id = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.eval()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    def placeholder_inference(prompt, mode):\n",
        "        return \"‚ö† Model loading failed. Please switch to GPU/High-RAM runtime.\"\n",
        "    inference_function = placeholder_inference\n",
        "    model = tokenizer = None\n",
        "\n",
        "# ‚úÖ Inference Function\n",
        "if model and tokenizer:\n",
        "    def granite_inference(prompt, mode):\n",
        "        try:\n",
        "            if mode == \"Eco-Query Assistant\":\n",
        "                formatted_prompt = (\n",
        "                    f\"Instruct: As a sustainable smart city assistant, answer this eco-sustainability question:\\n\"\n",
        "                    f\"{prompt}\\nOutput:\"\n",
        "                )\n",
        "            elif mode == \"Smart Complaint Resolver\":\n",
        "                formatted_prompt = (\n",
        "                    f\"Instruct: As a smart city complaint resolver, analyze this civic issue. \"\n",
        "                    f\"Classify it, suggest causes, and assign a department.\\n\"\n",
        "                    f\"Issue: {prompt}\\nOutput:\"\n",
        "                )\n",
        "            else:\n",
        "                formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
        "\n",
        "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_new_tokens=800,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Inference error: {e}\"\n",
        "\n",
        "    inference_function = granite_inference\n",
        "\n",
        "# ‚úÖ Gradio App with Login\n",
        "if inference_function:\n",
        "    with gr.Blocks(css=\"\"\"\n",
        "        .gradio-container {\n",
        "            background: linear-gradient(-45deg, #ff6ec4, #7873f5, #4ADEDE, #56FFA6);\n",
        "            background-size: 400% 400%;\n",
        "            animation: gradientFlow 15s ease infinite;\n",
        "            padding: 20px;\n",
        "            border-radius: 16px;\n",
        "        }\n",
        "\n",
        "        @keyframes gradientFlow {\n",
        "            0% { background-position: 0% 50%; }\n",
        "            50% { background-position: 100% 50%; }\n",
        "            100% { background-position: 0% 50%; }\n",
        "        }\n",
        "\n",
        "        .title {\n",
        "            font-size: 36px;\n",
        "            font-weight: bold;\n",
        "            text-align: center;\n",
        "            color: #ffffff;\n",
        "            text-shadow: 2px 2px 5px #000;\n",
        "        }\n",
        "\n",
        "        .subtitle {\n",
        "            font-size: 18px;\n",
        "            text-align: center;\n",
        "            color: #eeeeee;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "\n",
        "        #fancy-button {\n",
        "            background: linear-gradient(135deg, #8E2DE2, #4A00E0);\n",
        "            color: white !important;\n",
        "            font-weight: 700;\n",
        "            font-size: 17px;\n",
        "            padding: 14px 28px;\n",
        "            border-radius: 14px !important;\n",
        "            border: none;\n",
        "            box-shadow: 0 5px 15px rgba(138, 43, 226, 0.4);\n",
        "            transition: all 0.3s ease-in-out;\n",
        "            text-transform: uppercase;\n",
        "            letter-spacing: 1px;\n",
        "        }\n",
        "\n",
        "        #fancy-button:hover {\n",
        "            background: linear-gradient(135deg, #4A00E0, #8E2DE2);\n",
        "            transform: scale(1.05);\n",
        "            box-shadow: 0 8px 20px rgba(72, 0, 255, 0.5);\n",
        "        }\n",
        "\n",
        "        textarea, input[type=\"text\"], input[type=\"password\"] {\n",
        "            border-radius: 10px !important;\n",
        "            background-color: #1e1e2f;\n",
        "            color: white;\n",
        "            border: 1px solid #444;\n",
        "        }\n",
        "    \"\"\") as demo:\n",
        "\n",
        "        login_state = gr.State(False)\n",
        "\n",
        "        def login(username, password):\n",
        "            if username == \"admin\" and password == \"1234\":\n",
        "                return gr.update(visible=False), gr.update(visible=True), True, \"\"\n",
        "            else:\n",
        "                return None, None, False, \"‚ùå Invalid credentials. Try again.\"\n",
        "\n",
        "        # Login Page\n",
        "        with gr.Column(visible=True) as login_page:\n",
        "            gr.Markdown(\"<div class='title'>üîê Login to Smart City Assistant</div>\")\n",
        "            username = gr.Textbox(label=\"Username\", placeholder=\"Enter username\")\n",
        "            password = gr.Textbox(label=\"Password\", type=\"password\", placeholder=\"Enter password\")\n",
        "            login_btn = gr.Button(\"üîì Login\", elem_id=\"fancy-button\")\n",
        "            login_error = gr.Markdown(\"\", visible=True)\n",
        "\n",
        "        # Main App Page\n",
        "        with gr.Column(visible=False) as main_app:\n",
        "            gr.Markdown(\"<div class='title'>üåÜ Sustainable Smart City Assistant</div>\")\n",
        "            gr.Markdown(\"<div class='subtitle'>Powered by IBM Granite LLM ‚Äî Ask eco-questions or report civic issues</div>\")\n",
        "\n",
        "            with gr.Row():\n",
        "                mode_choice = gr.Radio(\n",
        "                    [\"Eco-Query Assistant\", \"Smart Complaint Resolver\"],\n",
        "                    label=\"üß† Choose Functionality\",\n",
        "                    value=\"Eco-Query Assistant\"\n",
        "                )\n",
        "\n",
        "            input_text = gr.Textbox(\n",
        "                lines=5,\n",
        "                placeholder=\"Type your eco-question or civic complaint here...\",\n",
        "                label=\"üìù Your Query\"\n",
        "            )\n",
        "\n",
        "            output_text = gr.Textbox(label=\"ü§ñ Assistant Response\", interactive=False)\n",
        "\n",
        "            submit_button = gr.Button(\"üöÄ Generate Response\", elem_id=\"fancy-button\")\n",
        "\n",
        "            submit_button.click(\n",
        "                fn=inference_function,\n",
        "                inputs=[input_text, mode_choice],\n",
        "                outputs=output_text\n",
        "            )\n",
        "\n",
        "        login_btn.click(\n",
        "            login,\n",
        "            inputs=[username, password],\n",
        "            outputs=[login_page, main_app, login_state, login_error]\n",
        "        )\n",
        "\n",
        "    demo.launch(share=True)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùó Model failed to load. Use GPU/High-RAM in Colab.\")"
      ]
    }
  ]
}